# .github/workflows/update_data.yml

name: Scheduled Data Fetch

# Controls when the action will run.
# This runs on a schedule (every hour) AND allows you to run it manually.
on:
  workflow_dispatch: # Allows manual triggering from the GitHub UI
  schedule:
    # Runs at the beginning of every hour (e.g., 1:00, 2:00, 3:00)
    - cron: "0 * * * *"

jobs:
  build-and-commit-data:
    runs-on: ubuntu-latest # Use a standard Linux environment

    steps:
      # 1. Check out your repository's code
      - name: Checkout repository
        uses: actions/checkout@v3

      # 2. Set up Python environment
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10' # Match the Python version Render uses

      # 3. Install the required Python packages
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # 4. Run the data pipeline script to fetch data and create cache files
      - name: Run Data Pipeline
        run: python data_pipeline.py

      # 5. Commit the updated data_cache directory back to the repository
      - name: Commit and push if data changed
        run: |
          git config --global user.name "GitHub Actions Bot"
          git config --global user.email "actions@github.com"
          git add data_cache/
          # The following command will only commit if there are changes
          git diff --quiet && git diff --staged --quiet || (git commit -m " chore: Update financial data cache" && git push)
          echo "Data commit process finished."

